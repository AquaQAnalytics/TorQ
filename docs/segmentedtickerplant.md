## Segmented Tickerplant Documentation

**Introduction**

A key component of the TorQ framework has always been the Tickerplant (TP) process. This process is a slightly modified version of the process of the same name found in the KDB+ Tick framework, which receives ticks from a feedhandler, timestamps them, and publishes them to any subscribed processes such as a real-time database (RDB) while writing the updates to a log file on disk. While this process is perfectly functional, there is only really one configuration for it, and some users may prefer greater flexibility. To this end, the Segmented Tickerplant (STP) has been developed.

**Segmented Tickerplant**

The idea behind the STP was to create a process which retained all the functionality of the Tickerplant while adding flexibility in terms of logging and subscriptions. It is entirely backwards compatible, meaning that any processes that depend on a TP can equally utilise an STP without painful code changes. It can still be used to create Chained Tickerplants (CTPs), is still performance conscious and still timestamps the incoming data before publishing it to its subscribers.

What has been added are multiple logging modes, which allow the logs to be split and partitioned, and subscription modes, which alter how the data is batched and published, as well as error handling, which sends bad messages to a separate file and customisation options.

**Starting a Segmented Tickerplant process**

Starting an STP process is similar to starting a tickerplant, we need to have an updated process.csv that contains a line for the STP process like the one below. Optional flags such as `-.stplg.batchmode` and `-.stplg.errmode` can be added to change settings for the process.

```
localhost,${KDBBASEPORT}+103,segmentedtickerplant,stp1,${TORQAPPHOME}/appconfig/passwords/accesslist.txt,1,0,,,${KDBCODE}/processes/segmentedtickerplant.q,1,-schemafile ${TORQAPPHOME}/database.q -.stplg.batchmode immediate -.stplg.errmode 0 -t 1,q
```

The process can either be started using:

```shell
bash torq.sh start stp1 -csv path/to/process.csv
```

or:

```shell
q ${TORQHOME}/torq.q -proctype segmentedtickerplant -procname stp1 -procfile path/to/proces.csv -load ${KDBCODE}/processes/segmentedtickerplant.q
```

**Logging Modes**

The default TP logging behaviour is to write all updates to disk in a single log file. This can be unwieldy as the whole file needs to be played through when a process starts, which can become slow as the number of ticks increases, and if the file is corrupted all the data is impacted. To add more flexibility, the following logging modes have been added which are set with the `.stplg.multilog` variable. It should be noted that the `.stpm.metatable` table is saved as a q object to the STPLOG folder along with any error logs generated by the error mode.

- None:

  This mode is essentially the default TP behaviour, where all ticks across all tables for a given day are stored in a single file, eg. `database20201026154808`. This is the simplest form of logging as everything is in one place.
  
  ```
  stplogs
      ├──stp1_2020.11.05/
      │  ├── err20201105000000
      │  ├── stpmeta
      │  └── stp1_20201105000000
      └──stp1_2020.11.06
         ├── err20201106000000
         ├── stpmeta
         └── stp1_20201106000000
  ```

- Periodic:

  In this mode all the updates are stored in a the same file but the logs are rolled according to a custom period, set with `.stplg.multilogperiod`. For example, if the period is set to an hour a new log file will be created every hour and stored in a daily partitioned directory. This means that if a subscriber goes down, only the last hour of logs need to be replayed rather than everything so far that day, and that any log file corruptions will only affect that time period of data rather than the whole day.
  
  ```
   stplogs
      ├──stp1_2020.11.05/
      │  ├── err20201105000000
      │  ├── periodic20201105000000
      │  ├── periodic20201105010000
      │  ├── periodic20201105020000
      │  └── stpmeta
      └──stp1_2020.11.06
         ├── err20201106000000
         ├── periodic20201106000000
         ├── periodic20201106010000
         ├── periodic20201106020000
         └── stpmeta
  ```

- Tabular:

  This mode is similar to the default behaviour except that each table has its own log file which is rolled daily in the form `trade20201026154808`. This has similar benefits to the previous case where only the ticks for individual tables need to be replayed for the day, and that any file mishaps are confined to a single table's worth of updates.
  
  ```
   stplogs/
      ├── stp1_2020.11.05
      │   ├── err20201105000000
      │   ├── logmsg_20201105000000
      │   ├── packets_20201105000000
      │   ├── quote_20201105000000
      │   ├── quote_iex_20201105000000
      │   ├── stpmeta
      │   ├── trade_20201105000000
      │   └── trade_iex_20201105000000
      └── stp1_2020.11.06
          ├── err20201106000000
          ├── logmsg_20201106000000
          ├── packets_20201106000000
          ├── quote_20201106000000
          ├── quote_iex_20201106000000
          ├── stpmeta
          ├── trade_20201106000000
          └── trade_iex_20201106000000
  ```

- Tabperiod:

  As the name suggests this mode combines the behaviour of the tabular and periodic logging modes, whereby each table has its own log file, each of which are rolled periodically as defined in the process. This adds the flexibility of both those modes when it comes to replays and file corruption too.
  
  ```
  stplogs/
      ├── stp1_2020.11.05
      │   ├── err20201105000000
      │   ├── err20201105010000
      │   ├── logmsg_20201105000000
      │   ├── logmsg_20201105010000
      │   ├── packets_20201105000000
      │   ├── packets_20201105010000
      │   ├── quote_20201105000000
      │   ├── quote_20201105010000
      │   ├── quote_iex_20201105000000
      │   ├── quote_iex_20201105010000
      │   ├── stpmeta
      │   ├── trade_20201105000000
      │   ├── trade_20201105010000
      │   ├── trade_iex_20201105000000
      │   └── trade_iex_20201105010000
      └── stp1_2020.11.06
          ├── err20201106000000
          ├── err20201106010000
          ├── logmsg_20201106000000
          ├── logmsg_20201106010000
          ├── packets_20201106000000
          ├── packets_20201106010000
          ├── quote_20201106000000
          ├── quote_20201106010000
          ├── quote_iex_20201106000000
          ├── quote_iex_20201106010000
          ├── stpmeta
          ├── trade_20201106000000
          ├── trade_20201106010000
          ├── trade_iex_20201106000000
          └── trade_iex_20201106010000
  ```

- Custom

  This mode allows the user to have more granular control over how each table is logged. The variable `.stplg.customcsv` points to a CSV file containing two columns, table and mode, and this allows the user to decide which logging mode to use for each table. An example CSV is below:

  ```
  table,mode
  trade,periodic
  trade_iex,periodic
  quote,tabular
  quote_iex,tabluar
  heartbeat,tabperiod
  ```

  Here we have the trade and trade_iex tables both being saved to the same periodic log file, the quote and quote_iex tables both having their own daily log file and the heartbeat table having a periodic log file all to itself. This mode may be advantageous in the case where some tables receive far more updates than others, so they can have more rigorously partitioned logs, and the sparser tables can be pooled together. There is some complexity associated with this mode, as there can be different log files rolling at different times.

**Batching Modes**

The other main update is how updates are published to subscribers. Again, there are named modes which are set with the `.stplg.batchmode` variable and these allow the user to be flexible with process latency and throughput by altering the `.u.upd` and `.z.ts` functions:

- Defaultbatch:

  This is effectively the standard TP batching mode where, upon receiving a tick, the STP immediately logs it to disk and batches the update which is published to subscribers whenever the timer function is next called. This mode represents a good balance of latency and overall throughput.

- Immediate:

  In this mode no batching occurs, and the update is logged and published immediately upon entering the STP. This is less efficient in terms of overall throughput but ensures low latency.

- Memorybatch:

  In this mode, neither logging nor publishing happens immediately but everything is held in memory until the timer function is called, at which point the update is logged and published. High overall message throughput is possible with this mode, but there is a risk that some messages aren't logged in the case of STP failure.

**Performance Comparison**

A custom performance stack was set up comprising a feed, a consumer, an STP, a vanilla TP and a KX Tick process along with an observer process which was responsible for coordinating the tests and processing the results. When the tests begin, the feed pushes single row updates to the selected TP process in a loop for one minute before pushing updates in batches of 100 rows for one minute. The observer then collects the results from the consumer which is subscribed to the TP and clears the table before resetting things so that the feed is pointing at either the same process in a different batching mode or a new process. In this way all the process modes are tested, including the immediate and batched modes for the TP and tick processes. 

These tests were run on a shared host with dual Intel Xeon Gold 6128 CPUs with a total of 12 cores and 24 threads with 128GB of memory. The results for single updates can be seen below:

| Process    | Batch Mode    | Max mps | Average mps |
| ---------- | ------------- | :-----: | :---------: |
| STP        | Default batch | 109,000 |   103,000   |
| STP        | Immediate     | 97,000  |   89,000    |
| STP        | Memory batch  | 185,000 |   174,000   |
| Vanilla TP | Immediate     | 83,000  |   75,000    |
| Vanilla TP | Batch         | 112,000 |   98,000    |
| Tick       | Immediate     | 95,000  |   87,000    |
| Tick       | Batch         | 111,000 |   103,000   |

And the following are for batched updates:

| Process    | Batching Mode |  Max mps  | Average mps |
| ---------- | ------------- | :-------: | :---------: |
| STP        | Default batch | 2,167,000 |  1,899,000  |
| STP        | Immediate     | 2,035,000 |  1,803,000  |
| STP        | Memory batch  | 2,473,000 |  2,115,000  |
| Vanilla TP | Immediate     | 2,034,000 |  1,776,000  |
| Vanilla TP | Batch         | 1,994,000 |  1,805,000  |
| Tick       | Immediate     | 1,941,000 |  1,722,000  |
| Tick       | Batch         | 2,050,000 |  1,872,000  |

The first obvious thing to be noticed is that batching the updates results in greater performance as there are fewer IPC operations and disk writes, and while some insight can be gleaned from these figures the single update results provide a better comparison of the actual process code performance. The memory batching mode is the clear leader in terms of raw performance as it does not write to disk on every update. The three 'default' batching modes are roughly equivalent in terms of performance and all have similar functionality. The three Immediate modes bring up the rear in terms of raw throughput, though the STP version is the performance leader here as it stores table column names in a dictionary which can be easily accessed rather than having to read the columns of a table in the root namespace.

When writing the code for these STP modes some compromise was taken between performance and maintenance. All the UPD functions are written in a standard way and have certain common elements abstracted away in namespaces, which does technically reduce performance. Also the effort made to ensure backwards compatibility means that certain left-field options were not taken advantage of, for example, sending enlisted lists from the Immediate mode rather than converting them to tables first. We will see later on how custom modes can be defined which can be more tailored to a given application.

**Error Trapping**

If the `.stplg.errmode` Boolean variable is set to true, an error log is opened on start up and the `.u.upd` function is wrapped in an error trap, so that if a bad message is received, it is not published but instead sent to the error log. The advantage of this is that bad updates are not sent through or replayed into the subscribers, which could cause issues, and they are easier to find and debug.

This mode is really designed for development/testing purposes, it shouldn't be necessary for a stable production feed and will add a small overhead to each update.

**Time Zone Behaviour**

A key tickerplant function is to timestamp the incoming data before it gets published to the rest of the system. In order to maintain applications and data in different time zones, such as a UK-based application which processes US data, the STP works with the `.eodtime` namespace variables in much the same way as other TorQ processes. The key variable used is `.eodtime.dailyadj` but more information on setting up a TorQ process in a different time zone can be found [here](https://aquaqanalytics.github.io/TorQ/utilities/#eodtimeq).

**Chained STP**

A chained tickerplant (TP) is a TP that is subscribed to another TP like a chain of TPs hence the name. This is useful for systems that need to behave differently for different subscribers, for example if you have a slow subscriber. When using the Chained STP, all endofday/endofperiod messages still originate from the STP and are merely passed on to subscribers through the Chained STP. Also, the Chained STP process is dependent on the Segmented TP. Therefore, if the connection to the STP dies, the ChainedSTP process will die. 

With these new changes to the tickerplant, we have added new features to chained tickerplants as well. Under a typical tick system there is one TP log for the main TP for each day, if a CTP goes down or needs to replay data the replay must happen from the main TP. A chained STP can have it's own log file and be in a different logging mode than the main TP, e.g. top level has no batching and chained STP has memory batching, to allow greater flexability.

There are 3 different logging modes for the Chained STP:

- None: Chained STP does not create or access any log files.

- Create: Chained STP creates its own log files independent of the STP logs. Subscribers then access the chained STP log files during replays

- Parent: STP logs are passed to subscribers during replays. Chained STP does not create any logs itself 

**Customisation and Flexibility**

The STP has been designed with customisation in mind. To this end here are a couple of ways to tailor the process to suit a particular application. The first is utilising the fact that each table has its own UPD function, meaning that some additional processing, such as adding a sequence number or a time-zone offset, can be done in the STP itself rather than needing to be done in a separate process. This is done by altering the `.stplg.updtab` dictionary like so:

```q
// Apply a sequence number to 'tabname'
q).stplg.updtab[`tabname]:{(enlist(count first x)#y),(enlist(count first x)#(`long$ .stplg.seqnum)),x}

q) .stplg.updtab
quote   | {(enlist(count first x)#y),x}
trade   | {(enlist(count first x)#y),x}
tabname | {(enlist(count first x)#y),(enlist(count first x)#(`long$ .stplg.seqnum)),x}
...
```

A second way that the process can be tailored is to make custom batching modes. The batching behaviour depends on two functions: `.u.upd` and `.z.ts`. The former is called every time an update arrives in the STP and the latter whenever the timer function is called (eg. if the process is started with `-t 1000`, this will be called every second). In the default batching mode, `.u.upd` inserts the update into a local table and writes it to the log file, and `.z.ts` publishes the contents of the local table before clearing it. To customise these functions, the `.stplg.upd` and `.stplg.zts` dictionaries will need to be customised. For example, the default batching code looks like the following:

```q
\d .stplg

// Standard batch mode - write to disk immediately, publish in batches
upd[`defaultbatch]:{[t;x;now]
  t insert x:.stplg.updtab[t] . (x;now);
  `..loghandles[t] enlist(`upd;t;x);
  // track tmp counts, and add these after publish
  @[`.stplg.tmpmsgcount;t;+;1];
  @[`.stplg.tmprowcount;t;+;count first x];
 };

zts[`defaultbatch]:{
  // publish and clear all tables, increment counts
  .stpps.pubclear[.stpps.t];
  // after data has been published, updated the counts
  .stplg.msgcount+:.stplg.tmpmsgcount;
  .stplg.rowcount+:.stplg.tmprowcount;
  // reset temp counts
  .stplg.tmpmsgcount:.stplg.tmprowcount:()!();
 };

\d .
```

Once this is done, simply update `.stplg.batchmode` with the name of the new mode and start the process.

**Other Notes**

Not everything about the STP is exactly the same as the TP, a couple of things have been changed:

- All updates must be lists of lists, meaning that single updates must be enlisted.